{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMxtenmMRyQUyWLKS00q+1c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pray-ash911/Sentiment-Analysis-using-logistic-regression-and-distilbert/blob/main/Sentiment_analysis_using_two_diff_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXMERktM1HjG",
        "outputId": "c33693da-d742-404f-c9df-2a9e7593cafc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:42: SyntaxWarning: invalid escape sequence '\\ '\n",
            "<>:42: SyntaxWarning: invalid escape sequence '\\ '\n",
            "/tmp/ipython-input-510031580.py:42: SyntaxWarning: invalid escape sequence '\\ '\n",
            "  print(\"\\ Downloading NLTK data files...\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " All libraries imported successfully!\n",
            "\\ Downloading NLTK data files...\n",
            "NLTK data downloaded successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Error loading omw-eng: Package 'omw-eng' not found in\n",
            "[nltk_data]     index\n"
          ]
        }
      ],
      "source": [
        "# PART 1: SETUP AND INSTALLATION\n",
        "\n",
        "# Install required libraries\n",
        "!pip install transformers datasets torch scikit-learn matplotlib seaborn wordcloud pandas numpy nltk kagglehub -q\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import re\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "# Deep Learning libraries\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "# Text preprocessing libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Visualization\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "print(\" All libraries imported successfully!\")\n",
        "\n",
        "# Download ALL required NLTK data\n",
        "print(\"\\ Downloading NLTK data files...\")\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-eng', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "print(\"NLTK data downloaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2: DATASET LOADING\n",
        "\n",
        "\n",
        "print(\"Downloading Sentiment140 dataset from Kaggle...\")\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
        "print(f\"Dataset downloaded to: {path}\")\n",
        "\n",
        "# Find the CSV file in the downloaded directory\n",
        "csv_path = None\n",
        "for root, dirs, files in os.walk(path):\n",
        "    for file in files:\n",
        "        if file.endswith('.csv'):\n",
        "            csv_path = os.path.join(root, file)\n",
        "            print(f\"Found CSV file: {csv_path}\")\n",
        "            break\n",
        "    if csv_path:\n",
        "        break\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(csv_path,\n",
        "                 encoding='latin-1',\n",
        "                 header=None,\n",
        "                 names=['target', 'id', 'date', 'flag', 'user', 'text'])\n",
        "\n",
        "print(f\" Dataset loaded successfully → {df.shape[0]:,} tweets\")\n",
        "print(f\"   Columns: {list(df.columns)}\")\n",
        "\n",
        "# Rename 'target' to 'label' for consistency\n",
        "df = df.rename(columns={'target': 'label'})\n",
        "\n",
        "# Convert labels: 0=negative, 4=positive → 0=negative, 1=positive\n",
        "df['label'] = df['label'].map({0: 0, 4: 1})\n",
        "\n",
        "# Display dataset info\n",
        "print(\"\\ Dataset Information:\")\n",
        "print(f\"   Total samples: {len(df):,}\")\n",
        "print(f\"   Positive tweets (1): {df['label'].sum():,}\")\n",
        "print(f\"   Negative tweets (0): {len(df) - df['label'].sum():,}\")\n",
        "print(f\"   Class balance: {(df['label'].sum() / len(df) * 100):.1f}% positive\")\n",
        "\n",
        "# Show first few samples\n",
        "print(\"\\ Sample tweets:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ9o7AIu1RAj",
        "outputId": "a25a3b9b-f6b0-45e7-b458-c358c03b9470"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Sentiment140 dataset from Kaggle...\n",
            "Using Colab cache for faster access to the 'sentiment140' dataset.\n",
            "Dataset downloaded to: /kaggle/input/sentiment140\n",
            "Found CSV file: /kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\n",
            " Dataset loaded successfully → 1,600,000 tweets\n",
            "   Columns: ['target', 'id', 'date', 'flag', 'user', 'text']\n",
            "\\ Dataset Information:\n",
            "   Total samples: 1,600,000\n",
            "   Positive tweets (1): 800,000\n",
            "   Negative tweets (0): 800,000\n",
            "   Class balance: 50.0% positive\n",
            "\\ Sample tweets:\n",
            "   label          id                          date      flag             user  \\\n",
            "0      0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
            "1      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
            "2      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
            "3      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
            "4      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
            "\n",
            "                                                text  \n",
            "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
            "1  is upset that he can't update his Facebook by ...  \n",
            "2  @Kenichan I dived many times for the ball. Man...  \n",
            "3    my whole body feels itchy and like its on fire   \n",
            "4  @nationwideclass no, it's not behaving at all....  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e0owIzsq1p40"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}